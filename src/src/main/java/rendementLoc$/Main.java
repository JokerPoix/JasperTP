package rendementLoc$;

import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SaveMode;

import java.nio.file.Files;
import java.nio.file.Paths;

public class Main {
	public static void main(String[] args) {
	
	// Initialisation de Spark
    SparkSession spark = SparkSession.builder()
            .appName("rendementLoc$")
            .config("spark.master", "local[*]")  // Utilisation de tous les c≈ìurs CPU disponibles
            .config("spark.driver.memory", "16g")  // Augmenter la m√©moire allou√©e au driver
            .config("spark.executor.memory", "8g")  // Ajouter de la m√©moire aux ex√©cutors
            .config("spark.executor.cores", "4")  // Augmenter les c≈ìurs d'ex√©cution
            .config("spark.sql.shuffle.partitions", "16")  // Augmenter les partitions pour √©viter la saturation de m√©moire
            .getOrCreate();
    
    // D√©finition des chemins de fichiers
    String inputFolderPath = "src/main/resources/inputs";
    String inputEstimationLoyerPath = "src/main/resources/inputs/EstimationLoyer";
    String inputValeursFoncieresPath = "src/main/resources/inputs/ValeursFoncieres";
    String outputFolderPath = "src/main/resources/outputs";
    String csvCodePostalInseeReferences = inputFolderPath + "/correspondanceCodeInseeCodePostal.csv";;

        
    // üîÑ Fusion des fichiers EstimationLoyer (si non d√©j√† g√©n√©r√©)
    if (!Files.exists(Paths.get(outputFolderPath + "/estimationLoyer.csv"))) {
        Dataset<Row> estimationLoyerDF = DataAggregator.mergeCSVFromFolder(spark, inputEstimationLoyerPath);
        DataAggregator.saveAsCSV(estimationLoyerDF, outputFolderPath, "estimationLoyer.csv");
    } else {
        System.out.println("‚úÖ estimationLoyer.csv existe d√©j√†, pas de traitement.");
    }

    // üîÑ Conversion et fusion des fichiers txt (Valeurs Foncieres)
    if (!Files.exists(Paths.get(outputFolderPath + "/valeursFoncieres.csv"))) {
        Dataset<Row> valeursFoncieresDF = DataAggregator.convertAndMergeTXTtoCSV(spark, inputValeursFoncieresPath);
        DataAggregator.saveAsCSV(valeursFoncieresDF, outputFolderPath, "valeursFoncieres.csv");
    } else {
        System.out.println("‚úÖ valeursFoncieres.csv existe d√©j√†, pas de traitement.");
    }

    // Charger le fichier estimationLoyer.csv
    Dataset<Row> data = spark.read().option("header", "true").option("delimiter", ";").csv(outputFolderPath + "/estimationLoyer.csv");

    // Filtrer les lignes correspondant √† la r√©gion (par exemple, Occitanie)
    String regionCode = "76";  // Mettez ici le code de la r√©gion d√©sir√©e
    Dataset<Row> filteredData = DataCleaner.filterRegion(data, regionCode);

    // Sauvegarder les donn√©es filtr√©es dans un CSV unique (optionnel)
    DataCleaner.saveFilteredData(data, regionCode, outputFolderPath + "/estimationLoyerOccitanie.csv");

    // Calculer et sauvegarder les moyennes par d√©partement (√† partir du jeu d√©j√† filtr√©)
    DataTransformer.saveAverageByDepartmentAndYear(filteredData, outputFolderPath);

    // Calculer et sauvegarder les moyennes par ville (√† partir du jeu d√©j√† filtr√©)
    DataTransformer.saveAverageByCityAndYear(filteredData, outputFolderPath);

    Dataset<Row> aggregatedDF = DataCleaner.cleanAndAggregateValuesFonciere(spark, outputFolderPath + "/valeursFoncieres.csv", csvCodePostalInseeReferences);
    DataTransformer.addYearColumnAndSave(aggregatedDF, outputFolderPath, "valeursFoncieresCleanedWithYearSurfacesINSEEAndSums.csv");
    System.out.println("‚úÖ Les traitements sur les estimations loyers(filtrage et agr√©gations) ont √©t√© r√©alis√©s pour la r√©gion " + regionCode + ".");
  
    //Calcul du rendement locatif par d√©partement --

    // 1) Charger le CSV "valeursFoncieresCleanedWithYearSurfacesINSEEAndSums.csv"
    //    qui contient DEP, ann√©e, surface_reelle_bati_somme, valeur_fonciere_somme, etc.
    String cleanedVFFile = outputFolderPath + "/valeursFoncieresCleanedWithYearSurfacesINSEEAndSums.csv";
    Dataset<Row> aggregatedDFWithYear = spark.read()
            .option("header", "true")
            .option("delimiter", ";")
            .csv(cleanedVFFile);

    // 2) Charger le CSV "averageEstimationLocationByDept.csv"
    //    qui contient DEP, ann√©e, avg_loypredm2
    String avgLoyerDeptFile = outputFolderPath + "/averageEstimationLocationByDept.csv";
    Dataset<Row> avgDeptDF = spark.read()
            .option("header", "true")
            .option("delimiter", ";")
            .csv(avgLoyerDeptFile);

    // 3) Calculer le rendement locatif par d√©partement et par ann√©e
    Dataset<Row> yieldByDept = RentalYieldGenerator.computeRentalYieldByDepartment(aggregatedDFWithYear, avgDeptDF);

    // 4) Sauvegarder le r√©sultat dans un unique fichier CSV : "rentalYieldByDept.csv"

    // Sauvegarde du rendement locatif dans un fichier unique en r√©utilisant la m√©thode existante
    DataTransformer.saveAsSingleCSV(yieldByDept, outputFolderPath, "rentalYieldByDept.csv");


    System.out.println("‚úÖ Calcul du rendement locatif par d√©partement termin√©.");
    
    String avgLoyerCitytFile = outputFolderPath + "/averageEstimationLocationByCity.csv";
    Dataset<Row> avgCitytDF = spark.read()
            .option("header", "true")
            .option("delimiter", ";")
            .csv(avgLoyerCitytFile);
    
    Dataset<Row> yieldByCity = RentalYieldGenerator.computeRentalYieldByCity(aggregatedDFWithYear, avgCitytDF);

    DataTransformer.saveAsSingleCSV(yieldByCity, outputFolderPath, "rentalYieldByCity.csv");

    // ---- Partie pour ins√©rer les CSV dans la base de donn√©es via JDBC ----
    // Dans une BDD my sql connect√© en root on peut faire ca:
    // DROP USER IF EXISTS 'poix'@'localhost';
	// CREATE USER 'poix'@'localhost' IDENTIFIED BY 'poix';
	// GRANT ALL PRIVILEGES ON rendement.* TO 'poix'@'localhost';
	// FLUSH PRIVILEGES;

    
    String jdbcUrl = "jdbc:mysql://localhost:3306/rendement?useSSL=false&serverTimezone=UTC";
    java.util.Properties connectionProperties = new java.util.Properties();
    connectionProperties.put("user", "poix");
    connectionProperties.put("password", "poix");
    connectionProperties.put("driver", "com.mysql.cj.jdbc.Driver");

    Dataset<Row> rentalYieldByDeptDF = spark.read()
            .option("header", "true")
            .option("delimiter", ";")
            .csv(outputFolderPath + "/rentalYieldByDept.csv");

    rentalYieldByDeptDF.write()
            .mode(SaveMode.Overwrite)
            .jdbc(jdbcUrl, "rental_yield_by_department", connectionProperties);

    Dataset<Row> rentalYieldByCityDF = spark.read()
            .option("header", "true")
            .option("delimiter", ";")
            .csv(outputFolderPath + "/rentalYieldByCity.csv");

    rentalYieldByCityDF.write()
            .mode(SaveMode.Overwrite)
            .jdbc(jdbcUrl, "rental_yield_by_city", connectionProperties);

    System.out.println("‚úÖ rentalYieldByCity ins√©r√© dans la base de donn√©es.");

    spark.stop();
	}
}
	
	

